{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5200d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import nibabel\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from utils.ridge_tools import cross_val_ridge, corr\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transpose_zscore(file): \n",
    "    dat = nibabel.load(file).get_data()\n",
    "    dat = dat.T\n",
    "    return zscore(dat,axis = 0)\n",
    "\n",
    "def smooth_run_not_masked(data,smooth_factor):\n",
    "    smoothed_data = np.zeros_like(data)\n",
    "    for i,d in enumerate(data):\n",
    "        smoothed_data[i] = gaussian_filter(data[i], sigma=smooth_factor, order=0, output=None,\n",
    "                 mode='reflect', cval=0.0, truncate=4.0)\n",
    "    return smoothed_data\n",
    "\n",
    "def delay_one(mat, d):\n",
    "        # delays a matrix by a delay d. Positive d ==> row t has row t-d\n",
    "    new_mat = np.zeros_like(mat)\n",
    "    if d>0:\n",
    "        new_mat[d:] = mat[:-d]\n",
    "    elif d<0:\n",
    "        new_mat[:d] = mat[-d:]\n",
    "    else:\n",
    "        new_mat = mat\n",
    "    return new_mat\n",
    "\n",
    "def delay_mat(mat, delays):\n",
    "        # delays a matrix by a set of delays d.\n",
    "        # a row t in the returned matrix has the concatenated:\n",
    "        # row(t-delays[0],t-delays[1]...t-delays[last] )\n",
    "    new_mat = np.concatenate([delay_one(mat, d) for d in delays],axis = -1)\n",
    "    return new_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_ind(n, n_folds):\n",
    "    ind = np.zeros((n))\n",
    "    n_items = int(np.floor(n/n_folds))\n",
    "    for i in range(0,n_folds -1):\n",
    "        ind[i*n_items:(i+1)*n_items] = i\n",
    "    ind[(n_folds-1)*n_items:] = (n_folds-1)\n",
    "    return ind\n",
    "\n",
    "def TR_to_word_CV_ind(TR_train_indicator,SKIP_WORDS=20,END_WORDS=5176):\n",
    "    time = np.load('./data/fMRI/time_fmri.npy')\n",
    "    runs = np.load('./data/fMRI/runs_fmri.npy') \n",
    "    time_words = np.load('./data/fMRI/time_words_fmri.npy')\n",
    "    time_words = time_words[SKIP_WORDS:END_WORDS]\n",
    "        \n",
    "    word_train_indicator = np.zeros([len(time_words)], dtype=bool)    \n",
    "    words_id = np.zeros([len(time_words)],dtype=int)\n",
    "    # w=find what TR each word belongs to\n",
    "    for i in range(len(time_words)):                \n",
    "        words_id[i] = np.where(time_words[i]> time)[0][-1]\n",
    "        \n",
    "        if words_id[i] <= len(runs) - 15:\n",
    "            offset = runs[int(words_id[i])]*20 + (runs[int(words_id[i])]-1)*15\n",
    "            if TR_train_indicator[int(words_id[i])-offset-1] == 1:\n",
    "                word_train_indicator[i] = True\n",
    "    return word_train_indicator   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec41be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test is the full NLP feature\n",
    "# train/test_pca is the NLP feature reduced to 10 dimensions via PCA that has been fit on the training data\n",
    "# feat_dir is the directory where the NLP features are stored\n",
    "# train_indicator is an array of 0s and 1s indicating whether the word at this index is in the training set\n",
    "def get_nlp_features_fixed_length(layer, seq_len, feat_type, feat_dir, train_indicator, SKIP_WORDS=20, END_WORDS=5176):\n",
    "    \n",
    "    loaded = np.load(feat_dir + feat_type + '_length_'+str(seq_len)+ '_layer_' + str(layer) + '.npy')\n",
    "    if feat_type == 'elmo':\n",
    "        train = loaded[SKIP_WORDS:END_WORDS,:][:,:512][train_indicator]   # only forward LSTM\n",
    "        test = loaded[SKIP_WORDS:END_WORDS,:][:,:512][~train_indicator]   # only forward LSTM\n",
    "    elif feat_type == 'bert' or feat_type == 'transformer_xl' or feat_type == 'use':\n",
    "        train = loaded[SKIP_WORDS:END_WORDS,:][train_indicator]\n",
    "        test = loaded[SKIP_WORDS:END_WORDS,:][~train_indicator]\n",
    "    else:\n",
    "        print('Unrecognized NLP feature type {}. Available options elmo, bert, transformer_xl, use'.format(feat_type))\n",
    "    \n",
    "    pca = PCA(n_components=10, svd_solver='full')\n",
    "    pca.fit(train)\n",
    "    train_pca = pca.transform(train)\n",
    "    test_pca = pca.transform(test)\n",
    "\n",
    "    return train, test, train_pca, test_pca \n",
    "\n",
    "     \n",
    "def prepare_fmri_features(train_features, test_features, word_train_indicator, TR_train_indicator, SKIP_WORDS=20, END_WORDS=5176):\n",
    "        \n",
    "    time = np.load('./data/fMRI/time_fmri.npy')\n",
    "    runs = np.load('./data/fMRI/runs_fmri.npy') \n",
    "    time_words = np.load('./data/fMRI/time_words_fmri.npy')\n",
    "    time_words = time_words[SKIP_WORDS:END_WORDS]\n",
    "        \n",
    "    words_id = np.zeros([len(time_words)])\n",
    "    # w=find what TR each word belongs to\n",
    "    for i in range(len(time_words)):\n",
    "        words_id[i] = np.where(time_words[i]> time)[0][-1]\n",
    "        \n",
    "    all_features = np.zeros([time_words.shape[0], train_features.shape[1]])\n",
    "    all_features[word_train_indicator] = train_features\n",
    "    all_features[~word_train_indicator] = test_features\n",
    "        \n",
    "    p = all_features.shape[1]\n",
    "    tmp = np.zeros([time.shape[0], p])\n",
    "    for i in range(time.shape[0]):\n",
    "        tmp[i] = np.mean(all_features[(words_id<=i)*(words_id>i-1)],0)\n",
    "    tmp = delay_mat(tmp, np.arange(1,5))\n",
    "\n",
    "    # remove the edges of each run\n",
    "    tmp = np.vstack([zscore(tmp[runs==i][20:-15]) for i in range(1,5)])\n",
    "    tmp = np.nan_to_num(tmp)\n",
    "        \n",
    "    return tmp[TR_train_indicator], tmp[~TR_train_indicator]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44359058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_class_time_CV_fmri_crossval_ridge(data, predict_feat_dict,\n",
    "                                          regress_feat_names_list = [],method = 'kernel_ridge', \n",
    "                                          lambdas = np.array([0.1,1,10,100,1000]),\n",
    "                                          detrend = False, n_folds = 4, skip=5):\n",
    "    \n",
    "    nlp_feat_type = predict_feat_dict['nlp_feat_type']\n",
    "    feat_dir = predict_feat_dict['nlp_feat_dir']\n",
    "    layer = predict_feat_dict['layer']\n",
    "    seq_len = predict_feat_dict['seq_len']\n",
    "        \n",
    "        \n",
    "    n_words = data.shape[0]\n",
    "    n_voxels = data.shape[1]\n",
    "\n",
    "    ind = CV_ind(n_words, n_folds=n_folds)\n",
    "\n",
    "    corrs = np.zeros((n_folds, n_voxels))\n",
    "    acc = np.zeros((n_folds, n_voxels))\n",
    "    acc_std = np.zeros((n_folds, n_voxels))\n",
    "\n",
    "    all_test_data = []\n",
    "    all_preds = []\n",
    "    \n",
    "    \n",
    "    for ind_num in range(n_folds):\n",
    "        train_ind = ind!=ind_num\n",
    "        test_ind = ind==ind_num\n",
    "        \n",
    "        word_CV_ind = TR_to_word_CV_ind(train_ind)\n",
    "        \n",
    "        _,_,tmp_train_features,tmp_test_features = get_nlp_features_fixed_length(layer, seq_len, nlp_feat_type, feat_dir, word_CV_ind)\n",
    "        train_features,test_features = prepare_fmri_features(tmp_train_features, tmp_test_features, word_CV_ind, train_ind)\n",
    "        \n",
    "        # split data\n",
    "        train_data = data[train_ind]\n",
    "        test_data = data[test_ind]\n",
    "\n",
    "        # skip TRs between train and test data\n",
    "        if ind_num == 0: # just remove from front end\n",
    "            train_data = train_data[skip:,:]\n",
    "            train_features = train_features[skip:,:]\n",
    "        elif ind_num == n_folds-1: # just remove from back end\n",
    "            train_data = train_data[:-skip,:]\n",
    "            train_features = train_features[:-skip,:]\n",
    "        else:\n",
    "            test_data = test_data[skip:-skip,:]\n",
    "            test_features = test_features[skip:-skip,:]\n",
    "\n",
    "        # normalize data\n",
    "        train_data = np.nan_to_num(zscore(np.nan_to_num(train_data)))\n",
    "        test_data = np.nan_to_num(zscore(np.nan_to_num(test_data)))\n",
    "        all_test_data.append(test_data)\n",
    "        \n",
    "        train_features = np.nan_to_num(zscore(train_features))\n",
    "        test_features = np.nan_to_num(zscore(test_features)) \n",
    "        \n",
    "        start_time = tm.time()\n",
    "        weights, chosen_lambdas = cross_val_ridge(train_features,train_data, n_splits = 10, lambdas = np.array([10**i for i in range(-6,10)]), method = 'plain',do_plot = False)\n",
    "\n",
    "        preds = np.dot(test_features, weights)\n",
    "        corrs[ind_num,:] = corr(preds,test_data)\n",
    "        all_preds.append(preds)\n",
    "            \n",
    "        print('fold {} completed, took {} seconds'.format(ind_num, tm.time()-start_time))\n",
    "        del weights\n",
    "\n",
    "    return corrs, acc, acc_std, np.vstack(all_preds), np.vstack(all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classify_neighborhoods(Ypred, Y, n_class=20, nSample = 1000,pair_samples = [],neighborhoods=[]):\n",
    "    # n_class = how many words to classify at once\n",
    "    # nSample = how many words to classify\n",
    "\n",
    "    voxels = Y.shape[-1]\n",
    "    neighborhoods = np.asarray(neighborhoods, dtype=int)\n",
    "\n",
    "    import time as tm\n",
    "\n",
    "    acc = np.full([nSample, Y.shape[-1]], np.nan)\n",
    "    acc2 = np.full([nSample, Y.shape[-1]], np.nan)\n",
    "    test_word_inds = []\n",
    "\n",
    "    if len(pair_samples)>0:\n",
    "        Ypred2 = Ypred[pair_samples>=0]\n",
    "        Y2 = Y[pair_samples>=0]\n",
    "        pair_samples2 = pair_samples[pair_samples>=0]\n",
    "    else:\n",
    "        Ypred2 = Ypred\n",
    "        Y2 = Y\n",
    "        pair_samples2 = pair_samples\n",
    "    n = Y2.shape[0]\n",
    "    start_time = tm.time()\n",
    "    for idx in range(nSample):\n",
    "        \n",
    "        idx_real = np.random.choice(n, n_class)\n",
    "\n",
    "        sample_real = Y2[idx_real]\n",
    "        sample_pred_correct = Ypred2[idx_real]\n",
    "\n",
    "        if len(pair_samples2) == 0:\n",
    "            idx_wrong = np.random.choice(n, n_class)\n",
    "        else:\n",
    "            idx_wrong = sample_same_but_different(idx_real,pair_samples2)\n",
    "        sample_pred_incorrect = Ypred2[idx_wrong]\n",
    "\n",
    "        #print(sample_pred_incorrect.shape)\n",
    "\n",
    "        # compute distances within neighborhood\n",
    "        dist_correct = np.sum((sample_real - sample_pred_correct)**2,0)\n",
    "        dist_incorrect = np.sum((sample_real - sample_pred_incorrect)**2,0)\n",
    "\n",
    "        neighborhood_dist_correct = np.array([np.sum(dist_correct[neighborhoods[v,neighborhoods[v,:]>-1]]) for v in range(voxels)])\n",
    "        neighborhood_dist_incorrect = np.array([np.sum(dist_incorrect[neighborhoods[v,neighborhoods[v,:]>-1]]) for v in range(voxels)])\n",
    "\n",
    "\n",
    "        acc[idx,:] = (neighborhood_dist_correct < neighborhood_dist_incorrect)*1.0 + (neighborhood_dist_correct == neighborhood_dist_incorrect)*0.5\n",
    "\n",
    "        test_word_inds.append(idx_real)\n",
    "    print('Classification for fold done. Took {} seconds'.format(tm.time()-start_time))\n",
    "    return np.nanmean(acc,0), np.nanstd(acc,0), acc, np.array(test_word_inds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
